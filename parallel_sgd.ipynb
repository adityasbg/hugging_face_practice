{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parallel_sgd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse\n",
        "import numpy as np\n",
        "\n",
        "n=10 # number of features\n",
        "m=20000 # number of training examples\n",
        "\n",
        "X = scipy.sparse.random(m,n, density=.2).toarray() # Guarantees sparse grad updates\n",
        "real_w = np.random.uniform(0,1,size=(n,1)) # Define our true weight vector\n",
        "\n",
        "X = X/X.max() # Normalizing for training\n",
        "\n",
        "y = np.dot(X,real_w)"
      ],
      "metadata": {
        "id": "XO7H9TZxHSyE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing.sharedctypes import Array\n",
        "from ctypes import c_double\n",
        "import numpy as np\n",
        "\n",
        "coef_shared = Array(c_double, \n",
        "        (np.random.normal(size=(n,1)) * 1./np.sqrt(n)).flat,\n",
        "        lock=False) # Hogwild!\n",
        "w = np.frombuffer(coef_shared)\n",
        "w = w.reshape((n,1)) "
      ],
      "metadata": {
        "id": "TTdXU6KGHfcF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = .001\n",
        "def mse_gradient_step(X_y_tuple):\n",
        "    global w # Only for instructive purposes!\n",
        "    X, y = X_y_tuple # Required for how multiprocessing.Pool.map works\n",
        "    \n",
        "    # Calculate the gradient\n",
        "    err = y.reshape((len(y),1))-np.dot(X,w)\n",
        "    grad = -2.*np.dot(np.transpose(X),err)/ X.shape[0]\n",
        "\n",
        "    # Update the nonzero weights one at a time\n",
        "    for index in np.where(abs(grad) > .01)[0]:\n",
        "        coef_shared[index] -= learning_rate*grad[index,0]\n"
      ],
      "metadata": {
        "id": "ZZkJSLQsH9sL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=1\n",
        "examples=[None]*int(X.shape[0]/float(batch_size))\n",
        "for k in range(int(X.shape[0]/float(batch_size))):\n",
        "    Xx = X[k*batch_size : (k+1)*batch_size,:].reshape((batch_size,X.shape[1]))\n",
        "    yy = y[k*batch_size : (k+1)*batch_size].reshape((batch_size,1))\n",
        "    examples[k] = (Xx, yy) "
      ],
      "metadata": {
        "id": "y7fGCdqkIepq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool\n",
        "\n",
        "# Training with Hogwild!\n",
        "p = Pool(5)  \n",
        "p.map(mse_gradient_step, examples)\n",
        "\n",
        "print('Loss function on the training set:', np.mean(abs(y-np.dot(X,w))))\n",
        "print('Difference from the real weight vector:', abs(real_w-w).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI5uEEh5Ii1a",
        "outputId": "41d120ff-db20-4ed0-a8be-5884f5f53223"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function on the training set: 0.003771390334085725\n",
            "Difference from the real weight vector: 0.06341289619152381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "plAX5W76K3Eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}